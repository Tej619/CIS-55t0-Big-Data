{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "OPOmuf_6QqZS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "CIS 5570 Programming Assignment 2\n",
        "\n",
        "By: Brad Byard, Anthony Lewis, and Tejas Vaity"
      ],
      "metadata": {
        "id": "1IyXG6foQr-P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "SETUP ENVIRONMENT"
      ],
      "metadata": {
        "id": "32lU9k6LGWaZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "d_ip7rdAZVd6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0178fe31-80ea-41d8-c785-b90b54264d7e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gdown in /usr/local/lib/python3.12/dist-packages (5.2.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (from gdown) (4.13.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from gdown) (3.20.0)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.12/dist-packages (from gdown) (2.32.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from gdown) (4.67.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->gdown) (2.8)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->gdown) (4.15.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown) (2025.10.5)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown) (1.7.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install -q pyspark\n",
        "!pip install gdown"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download the files from drive"
      ],
      "metadata": {
        "id": "XezOp0eCMLOZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gdown\n",
        "matrixaddition = \"https://drive.google.com/file/d/1FTAQvWD39_O7PU_qqCOggDlwPrwbfOci/view?usp=drive_link\"\n",
        "matrixmultiplication = \"https://drive.google.com/file/d/1ZHwF0BRSIQd5K_HecAp0EmUk6YhAtzwa/view?usp=drive_link\"\n",
        "test1 = \"https://drive.google.com/file/d/19c6pJVAA17cG3zJKFTQ3l151SunqMT9Q/view?usp=drive_link\"\n",
        "test2 = \"https://drive.google.com/file/d/1NzyiR40Zv7M5iQJ5M2Ip8BYQc7MP2jVG/view?usp=drive_link\"\n",
        "test3 = \"https://drive.google.com/file/d/1MVoAq-ViT3KgbH345_FKmE1nq4biTKD1/view?usp=drive_link\""
      ],
      "metadata": {
        "id": "uyTYBA0HG_a8"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "gdown.download(matrixaddition, 'matrixaddition-W25.txt', quiet=False, fuzzy=True);\n",
        "gdown.download(matrixmultiplication, 'matrixmultiplication-W25.txt', quiet=False, fuzzy=True);\n",
        "gdown.download(test1, '3x3-test1.txt', quiet=False, fuzzy=True);\n",
        "gdown.download(test2, '4x5-test2.txt', quiet=False, fuzzy=True);\n",
        "gdown.download(test3, 'test3.txt', quiet=False, fuzzy=True);"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w-4DrHsqH7lA",
        "outputId": "bc56c9ff-c760-42e4-d3e3-14c0a18f3716"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1FTAQvWD39_O7PU_qqCOggDlwPrwbfOci\n",
            "To: /content/matrixaddition-W25.txt\n",
            "100%|██████████| 7.86M/7.86M [00:00<00:00, 168MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1ZHwF0BRSIQd5K_HecAp0EmUk6YhAtzwa\n",
            "To: /content/matrixmultiplication-W25.txt\n",
            "100%|██████████| 926k/926k [00:00<00:00, 107MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=19c6pJVAA17cG3zJKFTQ3l151SunqMT9Q\n",
            "To: /content/3x3-test1.txt\n",
            "100%|██████████| 164/164 [00:00<00:00, 521kB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1NzyiR40Zv7M5iQJ5M2Ip8BYQc7MP2jVG\n",
            "To: /content/4x5-test2.txt\n",
            "100%|██████████| 414/414 [00:00<00:00, 1.51MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1MVoAq-ViT3KgbH345_FKmE1nq4biTKD1\n",
            "To: /content/test3.txt\n",
            "100%|██████████| 87.0/87.0 [00:00<00:00, 286kB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "Ba7yjeibERSE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext, SparkConf\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "conf = SparkConf().setAppName('SparkWordCount')\n",
        "sc = SparkContext.getOrCreate(conf = conf)\n",
        "\n",
        "sqlContext = SparkSession.builder\\\n",
        "        .master(\"local\")\\\n",
        "        .appName(\"Colab\")\\\n",
        "        .config('spark.ui.port', '4050')\\\n",
        "        .getOrCreate()\n"
      ],
      "metadata": {
        "id": "bnK_ZlN0RhlR"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ASSIGNMENT SPECIFIC SETUP"
      ],
      "metadata": {
        "id": "5i-YqQEqGanc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "PART 1 - A generic map reduce algorithm for matrix addition of any size"
      ],
      "metadata": {
        "id": "JHxsKoTDRwEc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Expect input such as:\n",
        "#M,i,j,val or M i j val\n",
        "\n",
        "#expect input as file, test for two different matrix sizes 3x3 and 4x5\n",
        "#upload test files to projects as labeled txts below\n",
        "\n",
        "input_file_test1 = ('/content/3x3-test1.txt')\n",
        "input_file_test2 = ('/content/4x5-test2.txt')\n",
        "\n",
        "#turn input file into an RDD of strings such as [\"M, 0, 0, 5\", \"N, 0, 0, 6\", etc.]\n",
        "\n",
        "lines1 = sc.textFile(input_file_test1)\n",
        "lines2 = sc.textFile(input_file_test2)\n",
        "\n",
        "#------------------------MAP------------------------------------------------------\n",
        "# Split each line like ['M','0','0','5']\n",
        "# Convert into key/value pairs: ((row, col), value)\n",
        "\n",
        "#MAP1 - Split each line like ['M', '0', '0', '887'] Example below\n",
        "#MAP2 - 1. int(p[1]) Converts '0' to the number 0 for row index\n",
        "#       2. int(p[2]) Converts '0' to the number 0 for column index\n",
        "#       3. float(p[3]) Converts '887' to 887.0 for a numeric cell value\n",
        "#       4. (int(p[1]), int(p[2])) converts to a tuple like (0,0)\n",
        "#       5. ((int(p[1]), int(p[2])), float(p[3])) Convers to a spark Key value pair\n",
        "\n",
        "kvpairs1 = (lines1\n",
        "            .map(lambda l: l.replace(',', ' ').split())\n",
        "            .map(lambda p: ((int(p[1]), int(p[2])), float(p[3]))))\n",
        "\n",
        "kvpairs2 = (lines2\n",
        "            .map(lambda l: l.replace(',', ' ').split())\n",
        "            .map(lambda p: ((int(p[1]), int(p[2])), float(p[3]))))\n",
        "\n",
        "#Test - Output looks as expected\n",
        "#kvpairs1.collect()[:10]\n",
        "#kvpairs2.collect()[:10]\n",
        "\n",
        "\n",
        "#------------------------REDUCE----------------------------------------------------\n",
        "# Add values for same (row, col)\n",
        "# Creates new RDD where each key is unique and the value is sum of the matrix value at that position\n",
        "matrixSum1 = kvpairs1.reduceByKey(lambda a, b: a + b)\n",
        "matrixSum2 = kvpairs2.reduceByKey(lambda a, b: a + b)\n",
        "\n",
        "#Test - Output looks as expected\n",
        "#matrixSum1.collect()[:10]\n",
        "#matrixSum2.collect()[:10]\n",
        "\n",
        "#------------------------SORT-----------------------------------------------------\n",
        "matrixSumSorted1 = matrixSum1.sortBy(lambda kv: (kv[0][0], kv[0][1]))\n",
        "matrixSumSorted2 = matrixSum2.sortBy(lambda kv: (kv[0][0], kv[0][1]))\n",
        "\n",
        "#Test - Output looks as expected\n",
        "#matrixSumSorted1.collect()[:10]\n",
        "#matrixSumSorted2.collect()[:10]\n",
        "\n",
        "#------------------------FINAL OUTPUTS---------------------------------------------\n",
        "# Flatten RDD to (i, j, val) for easy to read CSV layout\n",
        "flatRDD1 = matrixSumSorted1.map(lambda kv: (kv[0][0], kv[0][1], kv[1]))\n",
        "flatRDD2 = matrixSumSorted2.map(lambda kv: (kv[0][0], kv[0][1], kv[1]))\n",
        "\n",
        "# Convert to DataFrame for CSV\n",
        "matrixDF1 = sqlContext.createDataFrame(flatRDD1, ['Row', 'Col', 'Value'])\n",
        "matrixDF2 = sqlContext.createDataFrame(flatRDD2, ['Row', 'Col', 'Value'])\n",
        "\n",
        "# Save to CSV\n",
        "matrixDF1.toPandas().to_csv('test1_matrixAddition.csv', index=False)\n",
        "matrixDF2.toPandas().to_csv('test2_matrixAddition.csv', index=False)\n",
        "\n",
        "# Save to TXT (Spark writes folder with part-00000)\n",
        "(matrixSumSorted1\n",
        " .map(lambda kv: f\"{kv[0][0]} {kv[0][1]} {kv[1]}\")\n",
        " .coalesce(1)\n",
        " .saveAsTextFile(\"test1_matrixAddition_txt\"))\n",
        "\n",
        "(matrixSumSorted2\n",
        " .map(lambda kv: f\"{kv[0][0]} {kv[0][1]} {kv[1]}\")\n",
        " .coalesce(1)\n",
        " .saveAsTextFile(\"test2_matrixAddition_txt\"))"
      ],
      "metadata": {
        "id": "yWRZ7ocgR7xI"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PART 2 - Map Reduce Alogirthm to add two matrices in given text file matrixaddition-W25.txt"
      ],
      "metadata": {
        "id": "0Ms410mYHD5O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Input the given file in Canvas for use for addition\n",
        "input_file_addition = ('matrixaddition-W25.txt')\n",
        "\n",
        "#format like\n",
        "# M 1 77 731\n",
        "# N 174 27 550\n",
        "\n",
        "#turn input file into an RDD of strings such as [\"M, 0, 0, 5\", \"N, 0, 0, 6\", etc.]\n",
        "\n",
        "lines = sc.textFile(input_file_addition)\n",
        "\n",
        "#Test - Output looks as expected\n",
        "#lines.collect()[:10]\n",
        "\n",
        "#split lines into spark key value pairs\n",
        "#output should look like ((row_index, column_index), cell_value) to prep for addition of matrices\n",
        "\n",
        "#------------------------MAP-------------------------------------------------------------------------------------\n",
        "\n",
        "#MAP1 - Split each line like ['M', '0', '0', '887'] Example below\n",
        "#MAP2 - 1. int(p[1]) Converts '0' to the number 0 for row index\n",
        "#       2. int(p[2]) Converts '0' to the number 0 for column index\n",
        "#       3. float(p[3]) Converts '887' to 887.0 for a numeric cell value\n",
        "#       4. (int(p[1]), int(p[2])) converts to a tuple like (0,0)\n",
        "#       5. ((int(p[1]), int(p[2])), float(p[3])) Convers to a spark Key value pair\n",
        "\n",
        "kvpairs   = (lines.map(lambda l: l.replace(',', ' ').split())\n",
        "                  .map(lambda p: ((int(p[1]), int(p[2])), float(p[3]))))\n",
        "\n",
        "#Test - Output looks as expected\n",
        "#kvpairs.collect()[:10]\n",
        "\n",
        "#------------------------REDUCE----------------------------------------------------------------------------------\n",
        "\n",
        "#REDUCE - Add up all the values for each (row, col) key\n",
        "#Creates new RDD where each key is unique and the value is sum of the matrix value at that position\n",
        "matrixSum = kvpairs.reduceByKey(lambda a, b: a + b)\n",
        "\n",
        "#Test - Output looks as expected\n",
        "#matrixSum.collect()[:10]\n",
        "\n",
        "#------------------------SORT------------------------------------------------------------------------------------\n",
        "\n",
        "matrixSumSorted = matrixSum.sortBy(lambda kv: (kv[0][0], kv[0][1]))\n",
        "\n",
        "#Test - Output looks as expected\n",
        "#matrixSumSorted.collect()[:10]\n",
        "\n",
        "#------------------------FINAL OUTPUT---------------------------------------------------------------------------\n",
        "\n",
        "#flatten for CSV to (i,j,value)\n",
        "flatRDD = matrixSumSorted.map(lambda kv: (kv[0][0], kv[0][1], kv[1]))\n",
        "\n",
        "# Convert the sort into DataFrame\n",
        "matrixDF = sqlContext.createDataFrame(flatRDD, ['Row', 'Col', 'Value'])\n",
        "\n",
        "# Save to CSV\n",
        "matrixDF.toPandas().to_csv('matrixAdditionFinal.csv', index=False)\n",
        "\n",
        "# Save to TXT\n",
        "(matrixSumSorted\n",
        " .map(lambda kv: f\"{kv[0][0]} {kv[0][1]} {kv[1]}\")\n",
        " .coalesce(1)\n",
        " .saveAsTextFile(\"matrixAdditionFinal.Txt\"))\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "aiaQ40tIGilQ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PART 3 - a generic map reduce algorithm for single-pass matrix multiplication"
      ],
      "metadata": {
        "id": "0YcgmgxZN4A6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_file3 = \"test3.txt\"  # input file (e.g., \"test3.txt\", \"matrixmultiplication-W25 - Trimmed.txt\")\n",
        "lines = sc.textFile(input_file3) # turn iput file into strings RDD"
      ],
      "metadata": {
        "id": "x8AhMleeN523"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# parse lines into (label, index, vals)\n",
        "def parse_with_index(rdd):\n",
        "    # keep source order to get row indices\n",
        "    indexed = rdd.zipWithIndex() # (line, index)\n",
        "    def parse(line, index):\n",
        "        parts = line.strip().split()\n",
        "        label = parts[0].upper()\n",
        "        vals = list(map(int, parts[1:]))\n",
        "        return (label, index, vals)\n",
        "    return indexed.map(lambda x: parse(x[0], x[1]))\n",
        "\n",
        "parsed = parse_with_index(lines)\n",
        "\n",
        "# split M and N while preserving row order for indices\n",
        "M_rows = (lines.filter(lambda s: s[:1].upper() == \"M\") # keep M lines in file order\n",
        "               .map(lambda s: list(map(int, s.split()[1:])))\n",
        "               .zipWithIndex()\n",
        "               .map(lambda x: (x[1], x[0])) ) # (i, m_row)\n",
        "\n",
        "N_rows = (lines.filter(lambda s: s[:1].upper() == \"N\")\n",
        "               .map(lambda s: list(map(int, s.split()[1:])))\n",
        "               .zipWithIndex()\n",
        "               .map(lambda x: (x[1], x[0])) ) # (k, n_row)\n",
        "\n",
        "# gather small shape metadata\n",
        "I = M_rows.count()\n",
        "K = len(M_rows.first()[1]) # number of columns in M\n",
        "J = len(N_rows.first()[1]) # number of columns in N\n",
        "\n",
        "# broadcast dims\n",
        "I_b = sc.broadcast(I)\n",
        "K_b = sc.broadcast(K)\n",
        "J_b = sc.broadcast(J)\n",
        "\n",
        "# prepare RDDs keyed by matrix coordinate:\n",
        "# M elements: (i, k, a_ik)\n",
        "M_entries = M_rows.flatMap(lambda iv:\n",
        "    [ ( (iv[0], k), iv[1][k] ) for k in range(K_b.value) ]) \\\n",
        "    .map(lambda x: ('A', x[0][0], x[0][1], x[1]))  # ('A', i, k, a)\n",
        "\n",
        "# N elements: (k, j, b_kj)\n",
        "N_entries = N_rows.flatMap(lambda kv:\n",
        "    [ ( (kv[0], j), kv[1][j] ) for j in range(J_b.value) ]) \\\n",
        "    .map(lambda x: ('B', x[0][0], x[0][1], x[1]))  # ('B', k, j, b)\n",
        "\n",
        "# mapper (single pass): replicate to ((i,j), labelled)\n",
        "# for each a_ik, test ((i,j), ('A', k, a_ik)) for all j\n",
        "A_labelled = M_entries.flatMap(lambda rec:\n",
        "    [ ( (rec[1], j), ('A', rec[2], rec[3]) ) for j in range(J_b.value) ])\n",
        "\n",
        "# for each b_kj, emit ((i,j), ('B', k, b_kj)) for all i\n",
        "B_labelled = N_entries.flatMap(lambda rec:\n",
        "    [ ( (i, rec[2]), ('B', rec[1], rec[3]) ) for i in range(I_b.value) ])\n",
        "\n",
        "# union and ReduceByKey over (i,j): join on k and sum a_ik * b_kj\n",
        "joined = A_labelled.union(B_labelled).groupByKey()\n",
        "\n",
        "def reduce_entry(items):\n",
        "    # items: iterable of ('A' or 'B', k, val)\n",
        "    A = {}\n",
        "    B = {}\n",
        "    for label, k, v in items:\n",
        "        if label == 'A':\n",
        "            A[k] = v\n",
        "        else:\n",
        "            B[k] = v\n",
        "    total = 0\n",
        "    # sum over k in intersection\n",
        "    for k in set(A.keys()) & set(B.keys()):\n",
        "        total += A[k] * B[k] # multiply each pair of entries for the same k\n",
        "    return total\n",
        "\n",
        "output = joined.mapValues(reduce_entry)  # ((i,j), c_ij)\n",
        "\n",
        "# order by i then j and print or save\n",
        "result_rows = (output.map(lambda ijv: (ijv[0][0], ijv[0][1], ijv[1]))\n",
        "    .groupBy(lambda t: t[0]) # group by i\n",
        "    .mapValues(lambda entries: [v for _, j, v in sorted(entries, key=lambda x: x[1])])\n",
        "    .sortByKey()\n",
        "    .values())\n",
        "    # .map(lambda row: \" \".join(map(str, row)))) # formatting each row (a list of numbers) for print\n",
        "\n",
        "# print to validate the algorithm reduces\n",
        "for row in result_rows.take(10):\n",
        " print(\" \".join(str(x) for x in row))"
      ],
      "metadata": {
        "id": "ZFd5RmWLRXDJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67657a26-1c8b-488c-d971-a41ae13e9359"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "173 44 86 233\n",
            "4263 1074 2355 7124\n",
            "661 196 382 1097\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Part 4: Multiplication of two matrices of shape (128, 256) and (256, 128)"
      ],
      "metadata": {
        "id": "bMxgKAyr_Ifp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_matrix_multiplication_file = \"matrixmultiplication-W25.txt\"  # input file matrixmultiplication-W25.txt\n",
        "matrix_multi_lines = sc.textFile(input_matrix_multiplication_file) # turn input file into strings RDD"
      ],
      "metadata": {
        "id": "9lTO-FZpfpMH"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mm_lines = sc.textFile(input_matrix_multiplication_file)\n",
        "\n",
        "def parse_line(line):\n",
        "    parts = line.strip().split()\n",
        "    matrix = parts[0].strip()\n",
        "    a = int(parts[1])\n",
        "    b = int(parts[2])\n",
        "    val = float(parts[3])\n",
        "    return (matrix, a, b, val)\n",
        "\n",
        "data = mm_lines.map(parse_line)\n",
        "\n",
        "def mapper(record):\n",
        "    matrix, a, b, val = record\n",
        "    if matrix == 'M':\n",
        "        return (b, ('M', a, val))\n",
        "    else:\n",
        "        return (a, ('N', b, val))\n",
        "\n",
        "mapped = data.map(mapper)\n",
        "grouped = mapped.groupByKey()\n",
        "\n",
        "def multiply_by_key(record):\n",
        "    j, values = record\n",
        "    M_vals, N_vals = [], []\n",
        "    for tag, idx, val in values:\n",
        "        if tag == 'M':\n",
        "            M_vals.append((idx, val))\n",
        "        else:\n",
        "            N_vals.append((idx, val))\n",
        "\n",
        "    results = []\n",
        "    for i, mval in M_vals:\n",
        "        for k, nval in N_vals:\n",
        "            results.append(((i, k), mval * nval))\n",
        "    return results\n",
        "\n",
        "\n",
        "products = grouped.flatMap(multiply_by_key)\n",
        "result = products.reduceByKey(lambda x, y: x + y)\n",
        "formatted = result.map(lambda x: f\"{x[0][0]} {x[0][1]} {x[1]}\")\n",
        "formatted.saveAsTextFile(\"matrixMultiplicationFinal.txt\")\n",
        "\n",
        "# Saving the output in CSV file\n",
        "df = formatted.map(lambda line: line.split()).map(lambda parts: (int(parts[0]), int(parts[1]), float(parts[2])))\n",
        "matrixDF = sqlContext.createDataFrame(df, ['Row', 'Col', 'Value'])\n",
        "\n",
        "# Save as CSV\n",
        "matrixDF.coalesce(1).write.csv(\"matrixMultiplicationFinal.csv\", header=True, mode=\"overwrite\")\n"
      ],
      "metadata": {
        "id": "FQb_JZAK_RtJ"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you want to run code again remove previous output files, cleanup directory and stop spark session"
      ],
      "metadata": {
        "id": "g2mMX8F4RISh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Clean up any previous Spark session to improve run speed (optional, delete this code block when not needed anymore)\n",
        "# sqlContext.stop()\n",
        "# sc.stop()\n",
        "# del sc"
      ],
      "metadata": {
        "id": "vk-Dgpq7akvW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import shutil\n",
        "# import os\n",
        "\n",
        "# output_dir = \"matrixAdditionFinal.Txt\"\n",
        "\n",
        "# def delete_files(directories):\n",
        "#   for directory_path in directories:\n",
        "#     if os.path.exists(directory_path):\n",
        "#       shutil.rmtree(directory_path)\n",
        "#       print(f\"Deleted directory: {directory_path}\")\n",
        "#     else:\n",
        "#       print(f\"Directory {directory_path} does not exist.\")\n",
        "\n",
        "\n",
        "\n",
        "# dirs_to_delete = [\"matrixAdditionFinal.Txt\", \"test1_matrixAddition_txt\", \"test2_matrixAddition_txt\",\"test3_matrixMultiplication\",\"matrix_result\",\"matrixMultiplicationFinal.txt\"]\n",
        "# delete_files(dirs_to_delete)"
      ],
      "metadata": {
        "id": "8ODHbFeWmbv-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}